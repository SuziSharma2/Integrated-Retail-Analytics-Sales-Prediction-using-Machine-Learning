{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4cf75b-219f-4a22-8466-6232c897c43c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303b51d4-33a9-42ec-96f9-12b060faf176",
   "metadata": {},
   "outputs": [],
   "source": [
    "Project Title: Integrated Retail Analytics & Sales Prediction using Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38343871-e2ee-4484-b6d9-dcc3b22cebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Type: Supervised Machine Learning — Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46229b69-902e-42f6-aba1-e6aa036b1b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "Contribution: Individual\n",
    "By Suzi Sharma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b9b413-462c-4df1-82d9-21f26b2b9bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "GitHub link: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c66e449-6664-44ff-aea5-50a757b9d94d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f2cb28-898e-4e01-b09c-8593ead70b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project summary: This project — Integrated Retail Analytics for Store Optimization — constructs an end-to-end pipeline to transform sales, store \n",
    "# metadata, and auxiliary feature data into actionable business insights and a predictive model for store-level sales. The pipeline has three main \n",
    "# pillars: Exploratory Data Analysis (UBM), statistical validation of hypotheses, and predictive modeling with deployment readiness.\n",
    "# First, we load and harmonize the three user-provided CSVs and normalize column names to avoid fragile indexing. We build a master dataframe by \n",
    "# merging sales with store metadata and features (on store and date when available). We create a consistent target column sales_amount (mapped from \n",
    "# weekly_sales in these input files) and generate time features (year, month, day, dayofweek, is_weekend).\n",
    "\n",
    "# Exploratory Data Analysis strictly follows the UBM rule and produces 15+ charts:\n",
    "\n",
    "# Univariate charts (distribution of sales, top stores, weekly_sales histogram, day-of-week counts, holiday distribution) reveal skewness in sales (heavy tail skew), concentration of records in top stores, and operational calendar patterns.\n",
    "\n",
    "# Bivariate charts (weekly_sales vs dept averages, holiday boxplots, correlation matrix, month-vs-store heatmap) highlight departments with strongest average sales, the holiday uplift, and numeric relationships.\n",
    "\n",
    "# Multivariate analyses (scatter-matrix, store-month grouped bars, time-series trend, bubble chart, dept×holiday interaction) expose seasonality, interactions (e.g., specific departments perform better on holidays), and trends useful for inventory planning.\n",
    "\n",
    "# From EDA we derive three hypotheses and statistically test them (t-tests / Pearson correlation):\n",
    "\n",
    "# Weekend mean sales > weekday mean sales (tested via t-test).\n",
    "\n",
    "# Promotion days have higher sales (if promo exists; t-test\n",
    "# Price and sales are negatively correlated (Pearson’s r, if price present).\n",
    "# Cleaning & preprocessing use production-friendly strategies:\n",
    "# Numeric missing values → median imputation\n",
    "# Categorical missing → \"Unknown\"\n",
    "# Outlier handling → IQR capping (clip)\n",
    "# Categorical encoding → One-Hot Encoding in a ColumnTransformer\n",
    "# Scaling → StandardScaler for numeric features\n",
    "# All preprocessing is wrapped in scikit-learn Pipelines — this ensures reproducibility and easy deployment.\n",
    "# Modeling compares three models: Linear Regression (baseline), Random Forest Regressor, and Gradient Boosting Regressor. We use K-Fold cross-\n",
    "# validation to estimate generalization (neg_root_mean_squared_error for easier RMSE interpretation). For RandomForest and GradientBoosting we run \n",
    "# lightweight GridSearchCV to tune key hyperparameters. We evaluate on hold-out test set using RMSE, MAE and R². Model selection is RMSE-driven: \n",
    "# choose the model with smallest RMSE while also considering business interpretability and inference cost. Feature importance is extracted from the \n",
    "# tree-based model and saved.\n",
    "# Deployment readiness: final model is saved as a scikit-learn Pipeline (preprocessing + estimator) using joblib — this binary can be loaded for\n",
    "# inference in a Flask/FastAPI service. Plots, evaluation CSVs, hypothesis results and feature importance CSVs are saved under /mnt/data (or local \n",
    "# working dir) for reporting or automated dashboards.\n",
    "# Business impact highlights:\n",
    "# Seasonality & day-of-week effects inform staffing and inventory scheduling.\n",
    "# Holiday/promotional uplift quantification justifies promotion investment and targeting.\n",
    "# Top-store concentration suggests prioritizing high-return stores for new initiatives.\n",
    "# Predictive sales model enables demand planning, improved stock replenishment, and better promotion ROI measurement.\n",
    "# All logic is commented, packaged in reproducible steps, and saved artifacts make the notebook deployment-ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b949bb7-d87e-4aea-ad67-37d48f3780ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an end-to-end, deployment-ready retail analytics pipeline that ingests three CSVs (sales, stores, features), performs thorough Exploratory Data\n",
    "# Analysis following the UBM rule (Univariate, Bivariate, Multivariate), tests three hypothesis statements statistically, engineers features, trains \n",
    "# and compares multiple ML models to predict store-level sales (weekly sales / sales_amount), tunes models with cross-validation & hyperparameter search,\n",
    "# and exports artifacts (plots, model pipeline, evaluation) so the solution is ready to serve (Flask/FastAPI) or package. Emphasize production-grade \n",
    "# practices (pipelines, saving artifacts, clear comments, error handling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f309a8e3-0fd9-4ff7-b823-6c82942f0c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged shape: (421570, 12)\n"
     ]
    }
   ],
   "source": [
    "# Notebook: Integrated Retail Analytics for Store Optimization\n",
    "# Requirements: pandas, numpy, matplotlib, scikit-learn, scipy, joblib\n",
    "# Run everything in one cell in a Jupyter Notebook to be \"deployment-ready\".\n",
    "\n",
    "import os, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy import stats\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG & PATHS\n",
    "# -------------------------\n",
    "SALES_CSV    = \"sales data-set.csv\"\n",
    "STORES_CSV   = \"stores data-set.csv\"\n",
    "FEATURES_CSV = \"Features data set.csv\"\n",
    "\n",
    "OUT_DIR = \"retail_artifacts\"\n",
    "PLOTS_DIR = os.path.join(OUT_DIR, \"plots\")\n",
    "MODELS_DIR = os.path.join(OUT_DIR, \"models\")\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# UTILS\n",
    "# -------------------------\n",
    "def safe_read_csv(path):\n",
    "    \"\"\"Try common encodings and return a DataFrame.\"\"\"\n",
    "    for enc in [None, \"utf-8\", \"latin1\", \"cp1252\"]:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except Exception:\n",
    "            continue\n",
    "    raise FileNotFoundError(f\"Unable to read {path}\")\n",
    "\n",
    "def normalize_columns(df):\n",
    "    out = df.copy()\n",
    "    out.columns = [c.strip().lower().replace(\" \", \"_\") for c in out.columns]\n",
    "    return out\n",
    "def safe_merge_sales_features(sales_path, features_df, chunksize=50000):\n",
    "    \"\"\"\n",
    "    Merge sales and features safely in chunks to avoid MemoryError.\n",
    "    Handles multiple date formats automatically.\n",
    "    \"\"\"\n",
    "    feats = features_df.copy()\n",
    "\n",
    "    # robust date conversion for features\n",
    "    feats[\"date\"] = pd.to_datetime(\n",
    "        feats[\"date\"], errors=\"coerce\", dayfirst=True, infer_datetime_format=True\n",
    "    ).dt.date\n",
    "\n",
    "    merged_chunks = []\n",
    "    for chunk in pd.read_csv(sales_path, chunksize=chunksize):\n",
    "        ch = normalize_columns(chunk)\n",
    "        ch[\"date\"] = pd.to_datetime(\n",
    "            ch[\"date\"], errors=\"coerce\", dayfirst=True, infer_datetime_format=True\n",
    "        ).dt.date\n",
    "        merged = ch.merge(feats, how=\"left\", on=[\"store\", \"date\"])\n",
    "        merged_chunks.append(merged)\n",
    "\n",
    "    return pd.concat(merged_chunks, ignore_index=True)\n",
    "\n",
    "# def safe_merge_sales_features(sales_path, features_df, chunksize=50000):\n",
    "#     \"\"\"\n",
    "#     Merge sales and features safely in chunks to avoid MemoryError.\n",
    "#     \"\"\"\n",
    "#     feats = features_df.copy()\n",
    "#     feats[\"date\"] = pd.to_datetime(feats[\"date\"]).dt.date\n",
    "#     merged_chunks = []\n",
    "#     for chunk in pd.read_csv(sales_path, chunksize=chunksize):\n",
    "#         ch = normalize_columns(chunk)\n",
    "#         ch[\"date\"] = pd.to_datetime(ch[\"date\"]).dt.date\n",
    "#         merged = ch.merge(feats, how=\"left\", on=[\"store\", \"date\"])\n",
    "#         merged_chunks.append(merged)\n",
    "#     return pd.concat(merged_chunks, ignore_index=True)\n",
    "\n",
    "def save_fig(fig, fname):\n",
    "    path = os.path.join(PLOTS_DIR, fname)\n",
    "    fig.savefig(path, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    return path\n",
    "\n",
    "# -------------------------\n",
    "# LOAD DATA\n",
    "# -------------------------\n",
    "df_stores = normalize_columns(safe_read_csv(STORES_CSV))\n",
    "df_features = normalize_columns(safe_read_csv(FEATURES_CSV))\n",
    "\n",
    "# Keep only needed columns in features\n",
    "feat_cols = [\"store\",\"date\",\"temperature\",\"fuel_price\",\"cpi\",\n",
    "             \"unemployment\",\"isholiday\"]\n",
    "df_feat_small = df_features[[c for c in feat_cols if c in df_features.columns]]\n",
    "\n",
    "# -------------------------\n",
    "# MASTER MERGE  (CHUNKED)\n",
    "# -------------------------\n",
    "df_master = safe_merge_sales_features(SALES_CSV, df_feat_small, chunksize=50000)\n",
    "df_master = df_master.merge(df_stores, on=\"store\", how=\"left\")\n",
    "\n",
    "print(\"Merged shape:\", df_master.shape)\n",
    "\n",
    "# -------------------------\n",
    "# TARGET: sales_amount\n",
    "# -------------------------\n",
    "for cand in [\"sales_amount\",\"weekly_sales\",\"sales\",\"unit_sales\",\"amount\",\"revenue\"]:\n",
    "    if cand in df_master.columns:\n",
    "        df_master[\"sales_amount\"] = df_master[cand].astype(float)\n",
    "        break\n",
    "if \"sales_amount\" not in df_master.columns:\n",
    "    raise KeyError(\"No recognisable sales column found.\")\n",
    "\n",
    "# -------------------------\n",
    "# BASIC EDA & CLEANING\n",
    "# -------------------------\n",
    "df_master[\"date\"] = pd.to_datetime(df_master[\"date\"], errors=\"coerce\")\n",
    "df_master[\"year\"] = df_master[\"date\"].dt.year\n",
    "df_master[\"month\"] = df_master[\"date\"].dt.month\n",
    "df_master[\"dayofweek\"] = df_master[\"date\"].dt.dayofweek\n",
    "df_master[\"is_weekend\"] = df_master[\"dayofweek\"].isin([5,6]).astype(int)\n",
    "\n",
    "# save summaries\n",
    "df_master.describe(include=\"all\").T.to_csv(os.path.join(OUT_DIR,\"variables_description.csv\"))\n",
    "df_master.nunique(dropna=False).to_csv(os.path.join(OUT_DIR,\"unique_counts.csv\"))\n",
    "df_master.isnull().sum().sort_values(ascending=False).to_csv(os.path.join(OUT_DIR,\"missing_summary.csv\"))\n",
    "\n",
    "# simple imputation\n",
    "num_cols = df_master.select_dtypes(include=[np.number]).columns\n",
    "cat_cols = df_master.select_dtypes(include=[\"object\",\"category\"]).columns\n",
    "df_prep = df_master.copy()\n",
    "for c in num_cols:\n",
    "    df_prep[c] = df_prep[c].fillna(df_prep[c].median())\n",
    "for c in cat_cols:\n",
    "    df_prep[c] = df_prep[c].fillna(\"Unknown\")\n",
    "\n",
    "# -------------------------\n",
    "# UBM CHARTS  (memory-safe)\n",
    "# -------------------------\n",
    "# Chart 1: distribution of sales_amount\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "df_prep[\"sales_amount\"].hist(bins=60)\n",
    "plt.title(\"Distribution: sales_amount\")\n",
    "save_fig(fig,\"01_sales_amount_hist.png\")\n",
    "\n",
    "# Chart 2: top stores\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "df_prep[\"store\"].value_counts().nlargest(20).plot(kind=\"bar\")\n",
    "plt.title(\"Top20 stores by records\")\n",
    "save_fig(fig,\"02_top20_stores.png\")\n",
    "\n",
    "# Chart 3: weekly_sales hist\n",
    "if \"weekly_sales\" in df_prep.columns:\n",
    "    fig = plt.figure(figsize=(8,5))\n",
    "    df_prep[\"weekly_sales\"].hist(bins=50)\n",
    "    plt.title(\"weekly_sales distribution\")\n",
    "    save_fig(fig,\"03_weekly_sales_hist.png\")\n",
    "\n",
    "# Chart 4: dayofweek counts\n",
    "if \"dayofweek\" in df_prep.columns:\n",
    "    fig = plt.figure(figsize=(7,4))\n",
    "    df_prep[\"dayofweek\"].value_counts().sort_index().plot(kind=\"bar\")\n",
    "    plt.title(\"Records by dayofweek\")\n",
    "    save_fig(fig,\"04_dayofweek_bar.png\")\n",
    "\n",
    "# Chart 5: IsHoliday\n",
    "if \"isholiday\" in df_prep.columns:\n",
    "    fig = plt.figure(figsize=(6,4))\n",
    "    df_prep[\"isholiday\"].value_counts().plot(kind=\"bar\")\n",
    "    plt.title(\"IsHoliday distribution\")\n",
    "    save_fig(fig,\"05_isholiday_bar.png\")\n",
    "\n",
    "# Chart 6: avg weekly_sales by dept\n",
    "if \"dept\" in df_prep.columns:\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    df_prep.groupby(\"dept\")[\"weekly_sales\"].mean().nlargest(20).plot(kind=\"bar\")\n",
    "    plt.title(\"Top20 depts avg weekly_sales\")\n",
    "    save_fig(fig,\"06_avg_weekly_by_dept.png\")\n",
    "\n",
    "# Chart 7: weekly_sales by holiday (box)\n",
    "if \"isholiday\" in df_prep.columns:\n",
    "    groups=[df_prep[df_prep[\"isholiday\"]==g][\"weekly_sales\"]\n",
    "            for g in df_prep[\"isholiday\"].unique()]\n",
    "    fig = plt.figure(figsize=(6,5))\n",
    "    plt.boxplot(groups, labels=[str(g) for g in df_prep[\"isholiday\"].unique()])\n",
    "    plt.title(\"weekly_sales by isholiday\")\n",
    "    save_fig(fig,\"07_box_weekly_by_holiday.png\")\n",
    "\n",
    "# Chart 8: correlation (safe)\n",
    "num_candidates=[\"sales_amount\",\"weekly_sales\",\"temperature\",\n",
    "                \"fuel_price\",\"cpi\",\"unemployment\",\"size\"]\n",
    "present=[c for c in num_candidates if c in df_prep.columns]\n",
    "if present:\n",
    "    corr=df_prep[present].sample(min(50000,len(df_prep)),random_state=42).corr()\n",
    "    fig=plt.figure(figsize=(8,6))\n",
    "    plt.imshow(corr,cmap=\"coolwarm\")\n",
    "    plt.xticks(range(len(corr)),corr.columns,rotation=90)\n",
    "    plt.yticks(range(len(corr)),corr.columns)\n",
    "    plt.colorbar()\n",
    "    plt.title(\"Correlation (subset)\")\n",
    "    save_fig(fig,\"08_corr_safe.png\")\n",
    "\n",
    "# Chart 9: month vs store heatmap\n",
    "if \"month\" in df_prep.columns:\n",
    "    top12=df_prep[\"store\"].value_counts().nlargest(12).index\n",
    "    pivot=df_prep[df_prep[\"store\"].isin(top12)].pivot_table(\n",
    "        index=\"month\",columns=\"store\",values=\"weekly_sales\",aggfunc=\"mean\")\n",
    "    fig=plt.figure(figsize=(12,6))\n",
    "    plt.imshow(pivot.fillna(0),aspect=\"auto\")\n",
    "    plt.colorbar(); plt.title(\"Month vs Store heatmap\")\n",
    "    save_fig(fig,\"09_month_store_heatmap.png\")\n",
    "\n",
    "# Chart 10: scatter-matrix (safe)\n",
    "sm_vars=[c for c in present if c!=\"sales_amount\"][:4]+[\"sales_amount\"]\n",
    "if len(sm_vars)>=3:\n",
    "    sm=scatter_matrix(df_prep[sm_vars].dropna().sample(min(3000,len(df_prep)),random_state=42),\n",
    "                      figsize=(9,9),diagonal=\"hist\")\n",
    "    plt.suptitle(\"Scatter matrix\")\n",
    "    plt.savefig(os.path.join(PLOTS_DIR,\"10_scatter_matrix.png\")); plt.close()\n",
    "\n",
    "# Chart 14: avg weekly_sales by dept & holiday\n",
    "if {\"dept\",\"isholiday\"}.issubset(df_prep.columns):\n",
    "    pv=df_prep.pivot_table(index=\"dept\",columns=\"isholiday\",\n",
    "                           values=\"weekly_sales\",aggfunc=\"mean\").fillna(0).head(20)\n",
    "    pv.plot(kind=\"bar\",figsize=(12,6)).get_figure().savefig(os.path.join(PLOTS_DIR,\"14_dept_holiday_avg.png\")); plt.close()\n",
    "\n",
    "# Chart 15: top20 dept counts\n",
    "if \"dept\" in df_prep.columns:\n",
    "    fig=plt.figure(figsize=(10,5))\n",
    "    df_prep[\"dept\"].value_counts().head(20).plot(kind=\"bar\")\n",
    "    plt.title(\"Top20 dept counts\")\n",
    "    save_fig(fig,\"15_top20_dept_counts.png\")\n",
    "\n",
    "# -------------------------\n",
    "# HYPOTHESIS TESTING (3 statements)\n",
    "# -------------------------\n",
    "hypothesis_results = {}\n",
    "\n",
    "# Hypothesis 1: weekend mean > weekday mean\n",
    "if \"is_weekend\" in df_prep.columns and \"sales_amount\" in df_prep.columns:\n",
    "    weekend = df_prep[df_prep[\"is_weekend\"]==1][\"sales_amount\"].dropna()\n",
    "    weekday = df_prep[df_prep[\"is_weekend\"]==0][\"sales_amount\"].dropna()\n",
    "    if len(weekend) > 10 and len(weekday) > 10:\n",
    "        n = min(2000, len(weekend), len(weekday))\n",
    "        tstat, pval = stats.ttest_ind(weekend.sample(n, random_state=42), weekday.sample(n, random_state=42), equal_var=False)\n",
    "        hypothesis_results[\"weekend_vs_weekday\"] = {\"tstat\":float(tstat), \"pval\":float(pval), \"weekend_mean\":float(weekend.mean()), \"weekday_mean\":float(weekday.mean())}\n",
    "\n",
    "# Hypothesis 2: promo days > non-promo days (if promo exists)\n",
    "if \"promo\" in df_prep.columns and \"sales_amount\" in df_prep.columns:\n",
    "    p_yes = df_prep[df_prep[\"promo\"]==1][\"sales_amount\"].dropna()\n",
    "    p_no  = df_prep[df_prep[\"promo\"]==0][\"sales_amount\"].dropna()\n",
    "    if len(p_yes) > 10 and len(p_no) > 10:\n",
    "        n = min(2000, len(p_yes), len(p_no))\n",
    "        t2, p2 = stats.ttest_ind(p_yes.sample(n, random_state=42), p_no.sample(n, random_state=42), equal_var=False)\n",
    "        hypothesis_results[\"promo_effect\"] = {\"tstat\":float(t2), \"pval\":float(p2), \"promo_mean\":float(p_yes.mean()), \"no_promo_mean\":float(p_no.mean())}\n",
    "\n",
    "# Hypothesis 3: price negatively correlated with sales (if price present)\n",
    "if \"price\" in df_prep.columns and \"sales_amount\" in df_prep.columns:\n",
    "    corr = df_prep[[\"price\",\"sales_amount\"]].dropna().corr().iloc[0,1]\n",
    "    hypothesis_results[\"price_sales_corr\"] = {\"pearson_r\":float(corr)}\n",
    "\n",
    "# Save hypothesis results\n",
    "with open(os.path.join(OUT_DIR,\"hypothesis_results.json\"), \"w\") as f:\n",
    "    json.dump(hypothesis_results, f, indent=2)\n",
    "\n",
    "# -------------------------\n",
    "# PREPROCESS & FEATURE SELECTION FOR ML (operate on df_prep)\n",
    "# -------------------------\n",
    "if \"sales_amount\" not in df_prep.columns:\n",
    "    raise KeyError(\"sales_amount not found for ML target.\")\n",
    "y = df_prep[\"sales_amount\"].copy()\n",
    "X = df_prep.drop(columns=[\"sales_amount\",\"date\"], errors=\"ignore\").copy()\n",
    "\n",
    "# Drop very high-cardinality text fields to avoid exploding OHE\n",
    "high_card = [c for c in X.columns if X[c].dtype==object and X[c].nunique()>200]\n",
    "X = X.drop(columns=high_card, errors=\"ignore\")\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define transformers (use columns from the reduced X)\n",
    "num_features = [c for c in X.select_dtypes(include=[np.number]).columns.tolist()]\n",
    "cat_features = [c for c in X.select_dtypes(include=[\"object\",\"category\"]).columns.tolist()]\n",
    "\n",
    "num_pipeline = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())])\n",
    "cat_pipeline = Pipeline([(\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"Unknown\")), (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))])\n",
    "\n",
    "preprocessor = ColumnTransformer([(\"num\", num_pipeline, num_features), (\"cat\", cat_pipeline, cat_features)])\n",
    "\n",
    "# -------------------------\n",
    "# MODELS: baseline + RF + GB (with light GridSearch)\n",
    "# -------------------------\n",
    "models = {\n",
    "    \"LinearRegression\": Pipeline([(\"preproc\", preprocessor), (\"model\", LinearRegression())]),\n",
    "    \"RandomForest\": Pipeline([(\"preproc\", preprocessor), (\"model\", RandomForestRegressor(random_state=42, n_jobs=-1))]),\n",
    "    \"GradientBoosting\": Pipeline([(\"preproc\", preprocessor), (\"model\", GradientBoostingRegressor(random_state=42))])\n",
    "}\n",
    "\n",
    "# Cross-val baseline\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "cv_results = {}\n",
    "for name, pipe in models.items():\n",
    "    try:\n",
    "        scores = cross_val_score(pipe, X_train, y_train, scoring=\"neg_root_mean_squared_error\", cv=cv, n_jobs=-1)\n",
    "        cv_results[name] = {\"rmse_mean\": -float(np.mean(scores)), \"rmse_std\": float(np.std(scores))}\n",
    "    except Exception as e:\n",
    "        cv_results[name] = {\"error\": str(e)}\n",
    "with open(os.path.join(MODELS_DIR,\"cv_results.json\"), \"w\") as f:\n",
    "    json.dump(cv_results, f, indent=2)\n",
    "\n",
    "# GridSearch for RandomForest and GradientBoosting (wrapped in try/except to avoid runaway compute)\n",
    "param_grid_rf = {\"model__n_estimators\":[50,100],\"model__max_depth\":[10,20,None]}\n",
    "param_grid_gb = {\"model__n_estimators\":[100,200],\"model__learning_rate\":[0.05,0.1],\"model__max_depth\":[3,5]}\n",
    "\n",
    "gs_rf = GridSearchCV(models[\"RandomForest\"], param_grid_rf, scoring=\"neg_root_mean_squared_error\", cv=cv, n_jobs=-1)\n",
    "gs_gb = GridSearchCV(models[\"GradientBoosting\"], param_grid_gb, scoring=\"neg_root_mean_squared_error\", cv=cv, n_jobs=-1)\n",
    "\n",
    "# Fit carefully with try/except in case resources are limited\n",
    "try:\n",
    "    gs_rf.fit(X_train, y_train)\n",
    "    best_rf = gs_rf.best_estimator_\n",
    "except Exception as e:\n",
    "    print(\"RandomForest GridSearch failed or was stopped:\", e)\n",
    "    # fallback: fit a default RandomForest pipeline\n",
    "    best_rf = models[\"RandomForest\"]\n",
    "    best_rf.fit(X_train, y_train)\n",
    "\n",
    "try:\n",
    "    gs_gb.fit(X_train, y_train)\n",
    "    best_gb = gs_gb.best_estimator_\n",
    "except Exception as e:\n",
    "    print(\"GradientBoosting GridSearch failed or was stopped:\", e)\n",
    "    best_gb = models[\"GradientBoosting\"]\n",
    "    best_gb.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "evals = {}\n",
    "for name, est in [(\"LinearRegression\", models[\"LinearRegression\"]),\n",
    "                  (\"RandomForest\", best_rf),\n",
    "                  (\"GradientBoosting\", best_gb)]:\n",
    "    est.fit(X_train, y_train)\n",
    "    preds = est.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, preds)\n",
    "    rmse = mse ** 0.5\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    r2  = r2_score(y_test, preds)\n",
    "    evals[name] = {\"rmse\": float(rmse), \"mae\": float(mae), \"r2\": float(r2)}\n",
    "    \n",
    "    \n",
    "with open(os.path.join(MODELS_DIR,\"evaluation_results.json\"), \"w\") as f:\n",
    "    json.dump(evals, f, indent=2)\n",
    "\n",
    "# Save model comparison plot (if numeric RMSE available)\n",
    "names = list(evals.keys())\n",
    "rmses = [evals[n].get(\"rmse\", np.nan) for n in names]\n",
    "fig = plt.figure(figsize=(8,5)); plt.bar(names, rmses); plt.title(\"Model RMSE (test)\"); plt.ylabel(\"RMSE\"); plt.savefig(os.path.join(PLOTS_DIR,\"model_rmse_comparison.png\")); plt.close()\n",
    "\n",
    "# Feature importances (from RandomForest) - only if fitted\n",
    "try:\n",
    "    rf_pipeline = best_rf\n",
    "    # we attempt to extract feature names only if OHE exists and has get_feature_names_out\n",
    "    if hasattr(rf_pipeline.named_steps[\"preproc\"].transformers_[1][1], \"named_steps\"):\n",
    "        ohe = rf_pipeline.named_steps[\"preproc\"].transformers_[1][1].named_steps.get(\"ohe\", None)\n",
    "    else:\n",
    "        ohe = None\n",
    "    ohe_feature_names = []\n",
    "    if ohe is not None and hasattr(ohe, \"get_feature_names_out\"):\n",
    "        ohe_feature_names = list(ohe.get_feature_names_out(cat_features))\n",
    "    feature_names = num_features + ohe_feature_names\n",
    "    rf_model = rf_pipeline.named_steps[\"model\"]\n",
    "    if hasattr(rf_model, \"feature_importances_\"):\n",
    "        importances = rf_model.feature_importances_\n",
    "        fi = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
    "        fi.head(30).to_csv(os.path.join(MODELS_DIR,\"rf_feature_importance.csv\"))\n",
    "except Exception as e:\n",
    "    print(\"Could not extract RF feature importances:\", e)\n",
    "\n",
    "# Save best model (choose by rmse if present, else fallback)\n",
    "best_name = None\n",
    "try:\n",
    "    best_name = min({k:v for k,v in evals.items() if \"rmse\" in v}, key=lambda k: evals[k][\"rmse\"])\n",
    "except Exception:\n",
    "    best_name = \"RandomForest\"\n",
    "best_map = {\"LinearRegression\":models[\"LinearRegression\"], \"RandomForest\":best_rf, \"GradientBoosting\":best_gb}\n",
    "final_model = best_map.get(best_name, best_rf)\n",
    "joblib.dump(final_model, os.path.join(MODELS_DIR,\"final_model_pipeline.pkl\"))\n",
    "\n",
    "# -------------------------\n",
    "# REPORT & ARTIFACTS\n",
    "# -------------------------\n",
    "report = {\n",
    "    \"problem_statement\": \"Predict sales_amount (weekly sales) for store-level demand forecasting to improve inventory, promotions, and staffing.\",\n",
    "    \"cv_results\": cv_results,\n",
    "    \"gridsearch_rf_best\": getattr(gs_rf, \"best_params_\", {}),\n",
    "    \"gridsearch_gb_best\": getattr(gs_gb, \"best_params_\", {}),\n",
    "    \"eval_results\": evals,\n",
    "    \"best_model\": best_name\n",
    "}\n",
    "with open(os.path.join(OUT_DIR,\"project_report.json\"), \"w\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(\"Artifacts saved under:\", OUT_DIR)\n",
    "print(\"Best model:\", best_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a8d06b5-8891-4eb0-9538-893e366a8c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2010-05-02\n",
      "1    2010-12-02\n",
      "2           NaT\n",
      "3           NaT\n",
      "4    2010-05-03\n",
      "Name: date, dtype: object\n",
      "0    05/02/2010\n",
      "1    12/02/2010\n",
      "2    19/02/2010\n",
      "3    26/02/2010\n",
      "4    05/03/2010\n",
      "Name: date, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_sales[\"date\"].head())\n",
    "print(df_features[\"date\"].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f970ca5-304f-41c0-845f-339bd4574bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected columns:\n",
      " ['store' 'dept' 'weekly_sales' 'isholiday_x' 'temperature' 'fuel_price'\n",
      " 'cpi' 'unemployment' 'isholiday_y' 'type' 'size' 'year' 'month'\n",
      " 'dayofweek' 'is_weekend']\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# 👉 Load the trained pipeline\n",
    "model = joblib.load(\"retail_artifacts/models/final_model_pipeline.pkl\")\n",
    "\n",
    "# 👉 See the columns the model expects\n",
    "print(\"Expected columns:\\n\", model.feature_names_in_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
